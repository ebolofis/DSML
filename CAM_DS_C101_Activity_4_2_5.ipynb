{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ebolofis/Data-Science-Machine-Learning/blob/main/CAM_DS_C101_Activity_4_2_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First things first** - please go to 'File' and select 'Save a copy in Drive' so that you have your own version of this activity set up and ready to use.\n",
        "Remember to update your Course 1 notebook with links to your own work once completed!"
      ],
      "metadata": {
        "id": "oUogH156n3Ps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2.5 Activity: Implementing dimensionality reduction\n",
        "\n",
        "## Scenario\n",
        "As an independent data professional, you have been contracted to evaluate the selling price of automobiles. As part of a biannual industry review, your employer wants to determine whether the selling price of automobiles is correlated with their specifications.\n",
        "\n",
        "You have been provided with a data set, **automobiles.csv**, containing the current selling prices and specifications from randomly selected manufacturers. The data set contains 205 rows and 26 features. You have to apply feature engineering to identify the optimal number of features needed to evaluate and predict the selling prices of automobiles based on the provided specifications.\n",
        "\n",
        "## Objective\n",
        "Apply dimension reduction with PCA (or the appropriate technique) and t-SNE in a real-world context.\n",
        "\n",
        "## Assessment criteria:\n",
        "By completing this activity, you will be able to provide evidence that you can:\n",
        "1. Implement PCA (or the appropriate technique) and t-SNE for dimensionality reduction.\n",
        "2. Apply PCA (or the appropriate technique) and t-SNE (with the appropriate distance metric) to real-world data for improved analysis and visualisation.\n",
        "3. Choose the right number of principal components/Factors to balance dimensionality reduction and information retention.\n",
        "4. Evaluate the limitations of PCA and t-SNE.\n",
        "5. Apply critical thinking skills to assessing dimensionality reduction techniques.\n",
        "6. Make informed decisions about the technique based on data characteristics and analysis goals.\n",
        "\n",
        "\n",
        "## Activity guidance:\n",
        "1. Import the relevant libraries for dimension reduction.\n",
        "2. Transform and visualise the data with PCA (or the appropriate technique) and t-SNE (with the appropriate distance metric).\n",
        "3. Visualise the perplexity of the data.\n",
        "4. Combine PCA (or the appropriate technique) and t-SNE and create a final visualisation.\n",
        "5. Jot down your thoughts on the accuracy of predicting selling price based on the specifications of automobiles."
      ],
      "metadata": {
        "id": "2m5S96xMh6Mf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Step 2: Import the automobiles.csv file (data set) from GitHub with a url.\n",
        "url = \"https://raw.githubusercontent.com/fourthrevlxd/cam_dsb/main/automobiles.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 3: Data Preprocessing\n",
        "# Drop non-numeric columns and handle missing values\n",
        "numeric_cols = df.select_dtypes(include=np.number).columns\n",
        "df_numeric = df[numeric_cols].dropna()\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df_numeric)\n",
        "\n",
        "# Step 4: PCA\n",
        "pca = PCA()\n",
        "pca.fit(scaled_data)\n",
        "\n",
        "# Explained variance ratio\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "cumulative_variance = np.cumsum(explained_variance)\n",
        "\n",
        "# Plot explained variance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(cumulative_variance)\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Cumulative Explained Variance vs. Number of Principal Components')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Choose number of components (e.g., 90% variance)\n",
        "n_components = np.argmax(cumulative_variance >= 0.90) + 1\n",
        "print(f\"Number of components to retain 90% variance: {n_components}\")\n",
        "\n",
        "pca_final = PCA(n_components=n_components)\n",
        "pca_transformed = pca_final.fit_transform(scaled_data)\n",
        "\n",
        "# Step 5: t-SNE\n",
        "# t-SNE with default parameters\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "tsne_transformed = tsne.fit_transform(scaled_data)\n",
        "\n",
        "# t-SNE with Gower distance. Due to computational expense and scikit-learn limitations, this would require a custom implementation or using a different library.\n",
        "# For simplicity, we will use the default Euclidean distance.\n",
        "\n",
        "# Visualize t-SNE\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(tsne_transformed[:, 0], tsne_transformed[:, 1])\n",
        "plt.xlabel('t-SNE Dimension 1')\n",
        "plt.ylabel('t-SNE Dimension 2')\n",
        "plt.title('t-SNE Visualization')\n",
        "plt.show()\n",
        "\n",
        "# Step 6: Combined PCA and t-SNE\n",
        "# Apply t-SNE to PCA-transformed data\n",
        "tsne_pca = TSNE(n_components=2, random_state=42)\n",
        "tsne_pca_transformed = tsne_pca.fit_transform(pca_transformed)\n",
        "\n",
        "# Visualize combined PCA and t-SNE\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(tsne_pca_transformed[:, 0], tsne_pca_transformed[:, 1])\n",
        "plt.xlabel('t-SNE Dimension 1 (PCA Reduced)')\n",
        "plt.ylabel('t-SNE Dimension 2 (PCA Reduced)')\n",
        "plt.title('t-SNE Visualization (PCA Reduced)')\n",
        "plt.show()\n",
        "\n",
        "# Step 7: Perplexity Visualization (t-SNE)\n",
        "perplexities = [5, 10, 30, 50, 100]\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, perplexity in enumerate(perplexities):\n",
        "    tsne_perp = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
        "    tsne_perp_transformed = tsne_perp.fit_transform(scaled_data)\n",
        "    plt.subplot(2, 3, i + 1)\n",
        "    plt.scatter(tsne_perp_transformed[:, 0], tsne_perp_transformed[:, 1])\n",
        "    plt.title(f'Perplexity = {perplexity}')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Step 8: Thoughts on Predicting Selling Price\n",
        "# PCA and t-SNE help visualize data and reduce dimensionality, which can be useful for feature selection and understanding data structure.\n",
        "# However, they don't directly predict selling price.\n",
        "# To predict selling price, you would need to use a regression model (e.g., linear regression, random forest regression).\n",
        "# The reduced features from PCA could be used as input to the regression model.\n",
        "# The accuracy of the prediction would depend on the correlation between the car specifications and the selling price, as well as the quality of the regression model.\n",
        "# Limitations:\n",
        "# PCA: Linear transformation, may not capture complex non-linear relationships. Sensitive to outliers.\n",
        "# t-SNE: Computationally expensive, sensitive to perplexity, stochastic, can only be used for visualization, not dimensionality reduction for modeling.\n",
        "# Gower Distance Consideration:\n",
        "# Gower distance is better suited for mixed data types (categorical and numerical), which this dataset originally had.\n",
        "# The initial data preprocessing in this script drops the categorical data. If the categorical data were kept, Gower distance would be more appropriate.\n",
        "# However, implementing Gower distance with t-SNE in scikit-learn is not directly supported.\n",
        "# A custom implementation or an alternative library would be needed."
      ],
      "metadata": {
        "id": "RJzHmjpyCROK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reflect\n",
        "Write a brief paragraph highlighting your process and the rationale to showcase critical thinking and problem-solving. Specify the limitations of PCA and t-SNE.\n",
        "\n",
        "> Select the pen from the toolbar to add your entry."
      ],
      "metadata": {
        "id": "ZAvsL6jXhtUy"
      }
    }
  ]
}